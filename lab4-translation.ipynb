{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pathlib\nimport random\nimport string\nimport re\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-18T14:17:02.380913Z","iopub.execute_input":"2023-01-18T14:17:02.381423Z","iopub.status.idle":"2023-01-18T14:17:02.389097Z","shell.execute_reply.started":"2023-01-18T14:17:02.381380Z","shell.execute_reply":"2023-01-18T14:17:02.387736Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"text_file = \"/kaggle/input/greekenglish-dataset/ell.txt\"","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:17:02.392127Z","iopub.execute_input":"2023-01-18T14:17:02.393024Z","iopub.status.idle":"2023-01-18T14:17:02.401785Z","shell.execute_reply.started":"2023-01-18T14:17:02.392968Z","shell.execute_reply":"2023-01-18T14:17:02.400755Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"with open(text_file) as f:\n    lines = f.read().split(\"\\n\")[:-1]\ntext_pairs = []\nfor line in lines:\n    eng, ell, smth = line.split(\"\\t\")\n    ell = \"[start] \" + ell + \" [end]\"\n    text_pairs.append((eng, ell))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:17:02.403624Z","iopub.execute_input":"2023-01-18T14:17:02.404149Z","iopub.status.idle":"2023-01-18T14:17:02.464075Z","shell.execute_reply.started":"2023-01-18T14:17:02.404099Z","shell.execute_reply":"2023-01-18T14:17:02.462530Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"for _ in range(5):\n    print(random.choice(text_pairs))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:17:02.466208Z","iopub.execute_input":"2023-01-18T14:17:02.466796Z","iopub.status.idle":"2023-01-18T14:17:02.475649Z","shell.execute_reply.started":"2023-01-18T14:17:02.466746Z","shell.execute_reply":"2023-01-18T14:17:02.474059Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"('She has five older brothers.', '[start] Έχει πέντε μεγαλύτερους αδερφούς. [end]')\n('He sometimes comes home late.', '[start] Μερικές φορές γυρίζει αργά σπίτι. [end]')\n('I think Tom is a very good player.', '[start] Πιστεύω ότι ο Θωμάς είναι πολύ καλός παίχτης. [end]')\n(\"You won't be alone.\", '[start] Δε θα είστε μόνη. [end]')\n('Where do you swim?', '[start] Πού κολυμπάτε; [end]')\n","output_type":"stream"}]},{"cell_type":"code","source":"random.shuffle(text_pairs)\nnum_val_samples = int(0.15 * len(text_pairs))\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples :]\n\nprint(f\"{len(text_pairs)} total pairs\")\nprint(f\"{len(train_pairs)} training pairs\")\nprint(f\"{len(val_pairs)} validation pairs\")\nprint(f\"{len(test_pairs)} test pairs\")","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:17:02.479396Z","iopub.execute_input":"2023-01-18T14:17:02.479938Z","iopub.status.idle":"2023-01-18T14:17:02.517118Z","shell.execute_reply.started":"2023-01-18T14:17:02.479898Z","shell.execute_reply":"2023-01-18T14:17:02.515320Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"17444 total pairs\n12212 training pairs\n2616 validation pairs\n2616 test pairs\n","output_type":"stream"}]},{"cell_type":"code","source":"strip_chars = string.punctuation + \"¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\nvocab_size = 15000\nsequence_length = 20\nbatch_size = 64\n\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n\n\neng_vectorization = TextVectorization(\n    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n)\nell_vectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1,\n    standardize=custom_standardization,\n)\ntrain_eng_texts = [pair[0] for pair in train_pairs]\ntrain_ell_texts = [pair[1] for pair in train_pairs]\neng_vectorization.adapt(train_eng_texts)\nell_vectorization.adapt(train_ell_texts)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:17:14.666866Z","iopub.execute_input":"2023-01-18T14:17:14.667396Z","iopub.status.idle":"2023-01-18T14:17:15.644784Z","shell.execute_reply.started":"2023-01-18T14:17:14.667357Z","shell.execute_reply":"2023-01-18T14:17:15.643455Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def format_dataset(eng, ell):\n    eng = eng_vectorization(eng)\n    ell = ell_vectorization(ell)\n    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ell[:, :-1],}, ell[:, 1:])\n\n\ndef make_dataset(pairs):\n    eng_texts, ell_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    ell_texts = list(ell_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ell_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset)\n    return dataset.shuffle(2048).prefetch(16).cache()\n\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:17:19.889657Z","iopub.execute_input":"2023-01-18T14:17:19.890126Z","iopub.status.idle":"2023-01-18T14:17:20.230062Z","shell.execute_reply.started":"2023-01-18T14:17:19.890090Z","shell.execute_reply":"2023-01-18T14:17:20.229000Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"for inputs, targets in train_ds.take(1):\n    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n    print(f\"targets.shape: {targets.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:17:25.325550Z","iopub.execute_input":"2023-01-18T14:17:25.326137Z","iopub.status.idle":"2023-01-18T14:17:25.567018Z","shell.execute_reply.started":"2023-01-18T14:17:25.326088Z","shell.execute_reply":"2023-01-18T14:17:25.565631Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"inputs[\"encoder_inputs\"].shape: (64, 20)\ninputs[\"decoder_inputs\"].shape: (64, 20)\ntargets.shape: (64, 20)\n","output_type":"stream"},{"name":"stderr","text":"2023-01-18 14:17:25.558030: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n        attention_output = self.attention(\n            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n        )\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(\n            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n        )\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n        )\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n        proj_output = self.dense_proj(out_2)\n        return self.layernorm_3(out_2 + proj_output)\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:17:30.345647Z","iopub.execute_input":"2023-01-18T14:17:30.346123Z","iopub.status.idle":"2023-01-18T14:17:30.376781Z","shell.execute_reply.started":"2023-01-18T14:17:30.346086Z","shell.execute_reply":"2023-01-18T14:17:30.375005Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"embed_dim = 256\nlatent_dim = 2048\nnum_heads = 8\n\nencoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\nencoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\nencoder = keras.Model(encoder_inputs, encoder_outputs)\n\ndecoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\nencoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\nx = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\nx = layers.Dropout(0.5)(x)\ndecoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\ndecoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n\ndecoder_outputs = decoder([decoder_inputs, encoder_outputs])\ntransformer = keras.Model(\n    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:17:44.095270Z","iopub.execute_input":"2023-01-18T14:17:44.095936Z","iopub.status.idle":"2023-01-18T14:17:44.961993Z","shell.execute_reply.started":"2023-01-18T14:17:44.095882Z","shell.execute_reply":"2023-01-18T14:17:44.960779Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"epochs = 5  # This should be at least 30 for convergence\n\ntransformer.summary()\ntransformer.compile(\n    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)\ntransformer.fit(train_ds, epochs=epochs, validation_data=val_ds)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:18:07.505015Z","iopub.execute_input":"2023-01-18T14:18:07.505874Z","iopub.status.idle":"2023-01-18T14:41:46.906172Z","shell.execute_reply.started":"2023-01-18T14:18:07.505807Z","shell.execute_reply":"2023-01-18T14:41:46.904853Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Model: \"transformer\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nencoder_inputs (InputLayer)     [(None, None)]       0                                            \n__________________________________________________________________________________________________\npositional_embedding_4 (Positio (None, None, 256)    3845120     encoder_inputs[0][0]             \n__________________________________________________________________________________________________\ndecoder_inputs (InputLayer)     [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntransformer_encoder_2 (Transfor (None, None, 256)    3155456     positional_embedding_4[0][0]     \n__________________________________________________________________________________________________\nmodel_5 (Functional)            (None, None, 15000)  12959640    decoder_inputs[0][0]             \n                                                                 transformer_encoder_2[0][0]      \n==================================================================================================\nTotal params: 19,960,216\nTrainable params: 19,960,216\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/5\n191/191 [==============================] - 285s 1s/step - loss: 1.6639 - accuracy: 0.3466 - val_loss: 1.4590 - val_accuracy: 0.4080\nEpoch 2/5\n191/191 [==============================] - 273s 1s/step - loss: 1.3805 - accuracy: 0.4276 - val_loss: 1.2973 - val_accuracy: 0.4665\nEpoch 3/5\n191/191 [==============================] - 271s 1s/step - loss: 1.2180 - accuracy: 0.4842 - val_loss: 1.2411 - val_accuracy: 0.4918\nEpoch 4/5\n191/191 [==============================] - 271s 1s/step - loss: 1.0990 - accuracy: 0.5291 - val_loss: 1.0757 - val_accuracy: 0.5454\nEpoch 5/5\n191/191 [==============================] - 271s 1s/step - loss: 1.0082 - accuracy: 0.5645 - val_loss: 1.0130 - val_accuracy: 0.5706\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f5795fd49d0>"},"metadata":{}}]},{"cell_type":"code","source":"ell_vocab = ell_vectorization.get_vocabulary()\nell_index_lookup = dict(zip(range(len(ell_vocab)), ell_vocab))\nmax_decoded_sentence_length = 20\n\n\ndef decode_sequence(input_sentence):\n    tokenized_input_sentence = eng_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = ell_vectorization([decoded_sentence])[:, :-1]\n        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = ell_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\n\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor _ in range(3):\n    input_sentence = random.choice(test_eng_texts)\n    translated = decode_sequence(input_sentence)\n    print(input_sentence, \"\\t\", translated)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:49:03.617594Z","iopub.execute_input":"2023-01-18T14:49:03.618087Z","iopub.status.idle":"2023-01-18T14:49:05.018216Z","shell.execute_reply.started":"2023-01-18T14:49:03.618048Z","shell.execute_reply":"2023-01-18T14:49:05.016842Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"I'm pretty dizzy right now. \t [start] Είμαι πολύ να μην κάνω εκεί [end]\nA person who chases two rabbits won't catch either. \t [start] Αν δεν ήταν μια φορά το κουτί [end]\nTom's fast. \t [start] Ο Τομ ήταν εκεί [end]\n","output_type":"stream"}]}]}